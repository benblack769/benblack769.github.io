# The economic problem of AI

Why do you use technology? Do you play with it? Perhaps you do. But, what about the technologies you are still using from 10 years ago? Are they just toys or something more? Are they something that fits into your life like a belt, tying things together, making your life a little easier, a little more convenient, a little brighter, a little less lonely?

If someone offered you the exclusive ability to use an AI system, that no one else had, when would you use it? Perhaps if it made it easier to get to work? Perhaps if you noticed it was better at you at sales predictions at work? (assuming you could pass that work as your own) Perhaps if it notified you about good deals on craigslist you might be interested in? Perhaps if it took notes for you, and edited the grammar and spelling mistakes for you? Perhaps if it read your emails for you and noted what is important?

Oh, wait, all these technologies already exist. In fact, you and me are thinking about them precisely because they already fit into our lives so well. If we had to upend our lives to adapt to technology, not only would it be difficult to imagine, but we probably wouldn't want to make that step if we had the choice! So our imagination of improvements are necessarily incremental, based off the cultural, institutional, and technological structures that already exist.

Then what is all the hype about? What is it that people are so excited about for the future of AI? Well, AI can learn. Learn to do these things  we already do, but so well that we can trust them and we are suddenly free to do something else. So they claim it can  learn to drive cars, or translate documents, or read resumes, or answer questions, or perform technical support so that we don't have to.
But if you are depending a system to do important work in our stead, you want to be assured it will do an acceptable job, right? And how will you be assured of that? Will it be acceptable for the system to be tested in real life scenarios for an extended period of like, like a clinical trial in medicine?

If it is, then won't the system have to be fixed in place during and after the test? Unchanging, meaning, not learning? If not, then how else would you come to trust the system? An alien system that is somehow fundamentally different than anything we have seen before?

So it seems as though to rely on an AI system, it has to stop learning, and therefore to stop being intelligent. Before you think this is some sort of paradox, recall humans have a similar system in place. Adult humans actually have fewer connections in their brains than infants, and instead focus on being reliable in the ways they are depended on, staying focused for long periods of time without distraction.

So an intelligent system that adapts and learns must transform into an expert system, which is solid, reliable, and useful. I argue that this is a difficult problem.

Ok, you might be wondering, what is so hard about that? It is just software, why can't you just have it learn until it becomes good enough that we can rely on it, then stop the learning processes, and just keep the execution processes? Sure, but how did it learn in the first place? How does that learning process align with its eventual goal?

Well, why not start out by learning directly from its final goal? That is the strategy of most machine learning approaches today. Have a dataset, or a process that generates data, and a reward function that tell the system how good of a job it did. Then the system adapts to that reward, and slowly gets better. This ideas was first popularized by a phsycologist Pavlov, and his experiments with dogs, which used reinforced conditioning to get them salivating at the sound of a bell. Talk to a psychologist, however, and you will quickly learn that this approach is fatally flawed. First, capturing all the reward information in a scalar, or even a small set of scalar simply isn't much signal to learn on, and will take a very long time to learn anything of use. Even if this problem is overcome,  the signal need to be reinforced again and again, or else it will be forgotten. And if you keep on needing to be reminded of old stuff, how do you have any hope of learning higher level and more abstract material? Seems hard.

Now, psychologists have another idea of how a system learns. The learning agent must have its own objectives, and absorbs the world into its ideas. It is constantly reinforcing its own objectives, allowing its most solid ideas to stabilize completely, and allowing it to build layers of abstraction on top of the lower level layers. Occasionally, the world pushes back, forcing the agent to understand its limitations, questions its objectives, and restructure itself to better accommodate reality. But it can learn these limitations precisely because it already has a mostly accurate understanding of the world. But ultimately, how do we trust a system with its own goals? How do we understand it well enough to train it for our needs? How did we design it in the first place when its learning is so distantly separated from our experience and knowledge of the world? How do we stop the chaotic processes of learning so we can test it, while allowing it to adapt to new problems it encounters while doing its job? Seems hard.

This is a problem of AI, the question of what are we trying to build in the first place? Do you want to make an intelligent system beyond our understanding and control (like a human)? Or an expert system that fits in a technological niche, but has a shallow understanding of the world? Ultimately this can be framed as an economic question, that is, what should we allocate resources towards? This is the economic problem of AI. 

Well, why not try both? Researchers will gain new understanding of intelligent systems, and how they can be turned into expert systems as they try many things, and clever entrepreneurs will invent new technologies which these expert systems can be used for. The researchers and entrepreneurs will talk to each other, learn which ideas ended up being good, and which fail, and slowly but surely, the good ideas will rise to the top, the bad ideas will sink down, hardware will improve, enabling new kinds of systems, and technology progresses. What is wrong with this story?

What is wrong is that researchers don't have many incentives to actually create more intelligent systems! Th dynamics of this process are not only very slow, but often pathological in that more effort may result in worse outcomes.

* Money ultimately comes from success on unsolved problems.
* An unsolved problem is something a task we can evaluate success on, but not automatically solve.
* Finding new problems is only relevant when these problems block an existing problem.
* Researchers ignore problems which are too difficult to solve, as it makes their work seem useless.
* More money means more people working, crowing out the heroic efforts of those who want to break free of the chains of inertia.

Do you see the problem here? The problem is that money is tied to success in creating expert systems, not intelligent ones. While prestige may be gained by creating intelligent systems, the addition of more money packs the field with practitioners who care little about intelligent systems, crowding out the theorists and making it harder for them to talk to each other. And the way practitioners go about solving these problems is incredibly slow, as the entire technology chain needs to advance in step. This
