node_types:
    idea:
        name: Idea
        color: black
    project:
        name: Project
        color: green
    algorithm:
        name: Algorithm
        color: blue
    problem:
        name: Problem
        color: red
nodes:
    temporal_difference:
        title: "Temporal Differences"
        description: "Off policy learning technique by Richard Stutton"
        type: idea
        tags: learning_target
    UPGO:
        title: "UPGO Self-imitation algorithm"
        description: "???"
        type: algorithm
        tags: learning_target
    v_trace:
        title: "Clipped importance sampling (V-trace)"
        description: "???"
        type: algorithm
        tags: learning_target
    strategy_statistics:
        title: "Strategy statistics"
        description: "An encoding of a strategy into some (possibly high dimentional) numeric form"
        type: idea
        tags: exploration
    memetic_algorithm:
        title: "Memetic Algorithm"
        description: "Also known as genetic local search, Lamarckian EAs, Baldwinian EAs, cultual algorithms"
        type: idea
        tags: evolution, exploration
    pop_based_training:
        title: "Population based training"
        description: "Training hyperparameters are set by population of evolving learners."
        type: idea
        tags: evolution
    q_learning:
        title: "Q Learning"
        description: "Learning by "
        type: idea
        tags: reinforcement
    dqn:
        title: "Deep Q Network"
        description: "A q-learning approach utlitizing deep neural networks"
        type: algorithm
        tags: reinforcement
    bootstrap_dqn:
        title: "Bootstrap DQN"
        description: "A exploration method for DQN based RL based off random value functions"
        type: algorithm
        tags: reinforcement,exploration
    dithering_exploration:
        title: "Dithering exploration"
        description: "Exploration strategies which rely on minor perterbations in action space"
        type: idea
        tags: exploration
    alphastar:
        title: "AlphaStar"
        type: project
        description: "Deepmind's Starcraft playing engine."
    off_policy_learning:
        title: "Off policy learning"
        description: "The problem of a policy learning from the consequences of actions that a slightly different policy took."
        type: idea
        tags: reinforcement
    curriculum_learning:
        title: "Curriculum learning"
        description: "The idea that learning is fastest when training is on carefully prepared dataset where difficulty increases with abililty. "
        type: idea
        tags: learning
    self_play:
        title: "Self-play"
        description: "In a game, a RL agent can compete with itself to produce learning scenarios"
        type: algorithm
        tags: learning
    fictitious_self_play:
        title: "Fictitious self-play"
        description: "Agent plays against itself and archives of its past policies while learning."
        type: algorithm
    prioritized_fictitious_self_play:
        title: "Prioritized fictitious self-play"
        description: "Prioritizes play against agents it does poorly against"
        type: algorithm
    ladder_based_play_matching:
        title: "Ladder based play matching"
        description: "Prioritizes play against agents of similar overall elo"
        type: algorithm
    disk_game:
        title: "Disk Game"
        description: "A differentiable version of Rock Paper Scisors"
        type: problem
    helmholtz_decomposition:
        title: "Helmholtz decomposition"
        description: "Any sufficiently smooth, rapidly decaying function in 3d is the sum of a curl and a divergence term."
        type: idea
    transitive_cyclic_decomposition:
        title: "Transitive Cyclic Decomposition"
        description: "Any zero-sum differentiable game can be decomposed into a cyclic (counterable) and a transitive (globally better) parts"
        type: idea
    response_to_nash:
        title: "Response to the Nash"
        description: "Strategies are updated with the best response to the Nash equilibria of the population."
        type: algorithm
    response_to_rectified_nash:
        title: "Response to the rectified Nash"
        description: "Each effective agent is trained against agents it beats or ties"
        type: algorithm
relations:
    - alphastar -> temporal_difference
    - alphastar -> strategy_statistics
    - pop_based_training -> memetic_algorithm
    - alphastar -> pop_based_training
    - bootstrap_dqn - dithering_exploration
    - bootstrap_dqn -> pop_based_training
    - q_learning <- dqn
    - bootstrap_dqn <- dqn
    - temporal_difference -> off_policy_learning
    - curriculum_learning <- self_play
    - alphastar -> prioritized_fictitious_self_play
    - self_play <- fictitious_self_play
    - fictitious_self_play <- prioritized_fictitious_self_play
    - self_play <- prioritized_fictitious_self_play
    - curriculum_learning <- ladder_based_play_matching
    - alphastar -> ladder_based_play_matching
    - alphastar -> self_play
    - response_to_nash -> transitive_cyclic_decomposition
    - response_to_rectified_nash -> transitive_cyclic_decomposition
    - fictitious_self_play -> transitive_cyclic_decomposition
    - prioritized_fictitious_self_play -> transitive_cyclic_decomposition
    - self_play -> transitive_cyclic_decomposition
    - disk_game <- prioritized_fictitious_self_play
    - disk_game <- response_to_rectified_nash
    - disk_game - transitive_cyclic_decomposition
