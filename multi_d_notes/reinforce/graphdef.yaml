node_types:
    idea:
        name: Idea
        color: black
    project:
        name: Project
        color: green
    algorithm:
        name: Algorithm
        color: blue
    problem:
        name: Problem
        color: red
    paper:
        name: Paper
        color: yellow
nodes:
    exploration_exploitation:
        title: "Exploration-Exploitation Tradeoff"
        description: "Exploration involves searching suboptimal paths, which in extremes will criple the agents ability to perform well."
        type: problem
    prediction_based_exploration:
        title: "Prediction based exploration"
        description: "Newness of an enviornment is estimated by how badly a learned predictor works."
        type: idea
    tv_problem:
        title: "TV addiction problem"
        description: "A curious agent can (like a human) be addicted to long strings of new information"
        type: problem
    video_game_addiction:
        title: "Video game addiction problem"
        description: "A self-rewarding agent can easily get stuck playing an interactive game where it does new things which are useless and give no real reward"
        type: problem
    next_state_prediction_curiosity:
        title: "Next-state prediction curiosity"
        description: "Predicting next state of enviornment is a difficult task, which this paper uses to simulate curiosity"
        type: algorithm
    task_diversity:
        title: "Task diversity"
        description: "Method of increasing skill diversity to encourage exploration"
        type: algorithm
    random_network_distilation:
        title: "Random Network Distilation"
        description: "A curiosity based exploration paradigm based on using randomized networks as a state hash function."
        type: algorithm
    exploring_as_self_play:
        title: "Exploration as self-play"
        description: "Exploration can be seen as a self play problem (List of papers)"
        type: idea
    temporal_difference:
        title: "Temporal Differences"
        description: "Off policy learning technique by Richard Stutton"
        type: idea
        tags: learning_target
    UPGO:
        title: "UPGO Self-imitation algorithm"
        description: "???"
        type: algorithm
        tags: learning_target
    v_trace:
        title: "Clipped importance sampling (V-trace)"
        description: "???"
        type: algorithm
        tags: learning_target
    strategy_statistics:
        title: "Strategy statistics"
        description: "An encoding of a strategy into some (possibly high dimentional) numeric form"
        type: idea
        tags: exploration
    memetic_algorithm:
        title: "Memetic Algorithm"
        description: "Also known as genetic local search, Lamarckian EAs, Baldwinian EAs, cultual algorithms"
        type: idea
        tags: evolution, exploration
    pop_based_training:
        title: "Population based training"
        description: "Training hyperparameters are set by population of evolving learners."
        type: idea
        tags: evolution
    q_learning:
        title: "Q Learning"
        description: "Learning by "
        type: idea
        tags: reinforcement
    dqn:
        title: "Deep Q Network"
        description: "A q-learning approach utlitizing deep neural networks"
        type: algorithm
        tags: reinforcement
    bootstrap_dqn:
        title: "Bootstrap DQN"
        description: "A exploration method for DQN based RL based off random value functions"
        type: algorithm
        tags: reinforcement,exploration
    dithering_exploration:
        title: "Dithering exploration"
        description: "Exploration strategies which rely on minor perterbations in action space"
        type: idea
        tags: exploration
    randomized_value_exploration:
        title: "Randomized value exploration"
        description: "Exploring an entire path based on the guidence of a randomized function"
        type: idea
        tags: exploration
    alphastar:
        title: "AlphaStar"
        type: project
        description: "Deepmind's Starcraft playing engine."
    off_policy_learning:
        title: "Off policy learning"
        description: "The problem of a policy learning from the consequences of actions that a slightly different policy took."
        type: idea
        tags: reinforcement
    curriculum_learning:
        title: "Curriculum learning"
        description: "The idea that learning is fastest when training is on carefully prepared dataset where difficulty increases with abililty. "
        type: idea
        tags: learning
    self_play:
        title: "Self-play"
        description: "In a game, a RL agent can compete with itself to produce learning scenarios"
        type: algorithm
        tags: learning
    fictitious_self_play:
        title: "Fictitious self-play"
        description: "Agent plays against itself and archives of its past policies while learning."
        type: algorithm
    prioritized_fictitious_self_play:
        title: "Prioritized fictitious self-play"
        description: "Prioritizes play against agents it does poorly against"
        type: algorithm
    ladder_based_play_matching:
        title: "Ladder based play matching"
        description: "Prioritizes play against agents of similar overall elo"
        type: algorithm
    disk_game:
        title: "Disk Game"
        description: "A differentiable version of Rock Paper Scisors"
        type: problem
    helmholtz_decomposition:
        title: "Helmholtz decomposition"
        description: "Any sufficiently smooth, rapidly decaying function in 3d is the sum of a curl and a divergence term."
        type: idea
    transitive_cyclic_decomposition:
        title: "Transitive Cyclic Decomposition"
        description: "Any zero-sum differentiable game can be decomposed into a cyclic (counterable) and a transitive (globally better) parts"
        type: idea
    response_to_nash:
        title: "Response to the Nash"
        description: "Strategies are updated with the best response to the Nash equilibria of the population."
        type: algorithm
    response_to_rectified_nash:
        title: "Response to the rectified Nash"
        description: "Each effective agent is trained against agents it beats or ties"
        type: algorithm
    effective_diversity:
        title: "Effective Diversity"
        description: "A reward based definition of strategic diversity in antisymmetric two player game"
        type: idea
    colonel_blotto:
        title: "Colonel Blotto"
        description: "Resource allocation game where every player allocates resources to every area and wins in an area if they allocate the most resources."
        type: problem
    smoothed_symetric_nash_diversity:
        title: "Diversity for uniform agents"
        description: "Diversity in a population of uniform agents can be obtained by compting the number of 'smoothed' nash in the game."
        type: idea
    diverse_two_player_multi_agent_games:
        title: "Diversity for two player multi-agent games"
        description: ""
        type: idea
    nash_matrix_estimation:
        title: "N-player game matrix estimation"
        description: ""
        type: algorithm
    active_thief_discovery:
        title: "Active thief discovery"
        description: "A discovery problem of a adversarial agent"
        type: problem
    nash_based_diversity:
        title: "Nash based Diversity"
        description: "Evaluating diversity based off properties of the nash equilibria of a game"
        type: idea
relations:
    - alphastar -> temporal_difference
    - alphastar -> strategy_statistics
    - pop_based_training -> memetic_algorithm
    - alphastar -> pop_based_training
    - active_thief_discovery -> effective_diversity
    - pop_based_training <- nash_based_diversity
    - nash_based_diversity <- effective_diversity
    - nash_based_diversity <- smoothed_symetric_nash_diversity
    - nash_based_diversity <- response_to_nash
    - nash_matrix_estimation <- nash_based_diversity
    - diverse_two_player_multi_agent_games <- smoothed_symetric_nash_diversity
    - diverse_two_player_multi_agent_games <- effective_diversity
    - effective_diversity <- response_to_rectified_nash
    - bootstrap_dqn -> pop_based_training
    - task_diversity -> pop_based_training
    - task_diversity -> exploration_exploitation
    - bootstrap_dqn -> randomized_value_exploration
    - q_learning <- dqn
    - bootstrap_dqn <- dqn
    - temporal_difference -> off_policy_learning
    - curriculum_learning <- self_play
    - alphastar -> prioritized_fictitious_self_play
    - self_play <- fictitious_self_play
    - fictitious_self_play <- prioritized_fictitious_self_play
    - self_play <- prioritized_fictitious_self_play
    - curriculum_learning <- ladder_based_play_matching
    - alphastar -> ladder_based_play_matching
    - ladder_based_play_matching -> pop_based_training
    - alphastar -> self_play
    - response_to_nash -> transitive_cyclic_decomposition
    - response_to_rectified_nash -> transitive_cyclic_decomposition
    - fictitious_self_play -> transitive_cyclic_decomposition
    - prioritized_fictitious_self_play -> transitive_cyclic_decomposition
    - self_play -> transitive_cyclic_decomposition
    - disk_game <- prioritized_fictitious_self_play
    - disk_game <- response_to_rectified_nash
    - disk_game - transitive_cyclic_decomposition
    - response_to_rectified_nash -> effective_diversity
    - prioritized_fictitious_self_play -> effective_diversity
    - colonel_blotto <- response_to_rectified_nash
    - colonel_blotto <- response_to_nash
    - exploring_as_self_play <- curriculum_learning
    - exploring_as_self_play <- self_play
    - exploration_exploitation <- dithering_exploration
    - exploration_exploitation <- randomized_value_exploration
    - exploration_exploitation <- prediction_based_exploration
    - tv_problem <- next_state_prediction_curiosity
    - tv_problem <- random_network_distilation
    - tv_problem <- video_game_addiction
    - prediction_based_exploration <- next_state_prediction_curiosity
    - prediction_based_exploration <- random_network_distilation
