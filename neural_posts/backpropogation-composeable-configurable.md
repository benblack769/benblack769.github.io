Backpropagation

{% raw %}
$$x + y = z$$
{% endraw %}

Neural networks have been praised as one of the most flexible machine learning tool, and indeed algorithm of all time. However, if you look more closely, neural networks for different problem domains have significantly different structure. So why  do people think they are flexible? What makes people think that they are so powerful?

The answer lies in the flexibility of the core algorithm: backpropagation of error.

At its core, backpropagation is simply the chain rule from calculus.

A standard neural network

Overviewewof chain rule.
DiagraDraTalk about composition of matricies.

Talk about how you can compose matricies natrually in trivial ways. Bring up Depp networks, m. Then talk about how convolutional networks are the a nontrivial version of this.

Another possible one is sintane decomposition, ie.e the function applied to each wor,d and the the binary output as a function of that concatenation of outputs.  Is a more trivial example.



The talk about LSTMs. Talk about how they are a non-trivial one, but rely on the same basic intuitions. Show ow LSTMs operate in much the same level as single matricies blocks.

Talk abou thow now people use LSTMS as building blocks for more onontirival things lik ethe

Talk about how the output is a beutiful algorithms that all do very differnet things , but baesed on fairly straitforward intution allowed by the composition properties of backpropogation.
