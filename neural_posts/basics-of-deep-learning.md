# Understanding the mechanics of deep learning

## Problems deep learning tries to solve

### Image Analysis: Text reading

Consider the CAPTCHA problem:

![captcha](/images/deep_learning_basics/recaptcha.png)


### Speech recognition

To see why speech is so hard, just look at it:

![sound-wave-img](/images/deep_learning_basics/sound_img/sound_zoomed.PNG)

This is the visual representation of this:

<audio controls>
  <source src="/images/deep_learning_basics/sound_img/hellomynameisben.m4a" type="audio/mpeg">
Your browser does not support the audio element.
</audio>

Now, if you know about sound, then you know that a lot of this information is really just overlapping frequencies and volumes which can be extracted and turned into a form which is easier to work with.

Unfortunately, humans can understand sounds with differnet

* Pitch/Frequency
* Volume/Amplitude
* Speed of changes (talking faster)
* Distorters

Not only that, but we humans are able to understand many different languages by learning them. So we already know that any model of reasonable complexity will work anyways.

### Requirements to solve these problems

* Learning
* Filtering out noise
* Abstracting shapes
* Compressing data 

## Key ideas of deep learning, and how they try to solve those problems

### Gradient descent

### NN function (activation/mat mul)

### Backpropagation

## Working example (MNIST)

## Issues with those ideas, and examples of fixes, and visualizations of how they work

## Overview of modern deep learning architectures (LSTM, convolution, attention, softmax probabilities)

## Solving non-standard problems like adversarial learning (if time)

## More resources

http://colah.github.io/

https://en.wikipedia.org/wiki/Activation_function







# old























## Introductory ideas

### Machine learning

### Complex natural functions

### Hierarchical abstraction

## Neural network algorithm

### Algebraic construction
*Diagram*

#### Input/output format
#### Weight matrixes - linear functions define abstractions/interactions
#### Activation function

### Backpropagation

#### Cost function

## Applications/examples

###




















* Deep learning and ANNs are not quite the same:  deep learning implies backpropogation, ANNs make no mention of that idea.

* More art than science
* So different than other ideas that we don't have terminology to talk about them (a whole new branch of science)
