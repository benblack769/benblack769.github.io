var node_js_info = [{"node": "exploration_exploitation", "title": "Exploration-Exploitation Tradeoff", "description": "Exploration involves searching suboptimal paths, which in extremes will criple the agents ability to perform well.", "type": "problem"}, {"node": "prediction_based_exploration", "title": "Prediction based exploration", "description": "Newness of an enviornment is estimated by how badly a learned predictor works.", "type": "idea"}, {"node": "tv_problem", "title": "TV addiction problem", "description": "A curious agent can (like a human) be addicted to long strings of new information", "type": "problem"}, {"node": "video_game_addiction", "title": "Video game addiction problem", "description": "A self-rewarding agent can easily get stuck playing an interactive game where it does new things which are useless and give no real reward", "type": "problem"}, {"node": "next_state_prediction_curiosity", "title": "Next-state prediction curiosity", "description": "Predicting next state of enviornment is a difficult task, which this paper uses to simulate curiosity", "type": "algorithm"}, {"node": "task_diversity", "title": "Task diversity", "description": "Method of increasing skill diversity to encourage exploration", "type": "algorithm"}, {"node": "random_network_distilation", "title": "Random Network Distilation", "description": "A curiosity based exploration paradigm based on using randomized networks as a state hash function.", "type": "algorithm"}, {"node": "exploring_as_self_play", "title": "Exploration as self-play", "description": "Exploration can be seen as a self play problem (List of papers)", "type": "idea"}, {"node": "temporal_difference", "title": "Temporal Differences", "description": "Off policy learning technique by Richard Stutton", "type": "idea", "tags": "learning_target"}, {"node": "UPGO", "title": "UPGO Self-imitation algorithm", "description": "???", "type": "algorithm", "tags": "learning_target"}, {"node": "v_trace", "title": "Clipped importance sampling (V-trace)", "description": "???", "type": "algorithm", "tags": "learning_target"}, {"node": "strategy_statistics", "title": "Strategy statistics", "description": "An encoding of a strategy into some (possibly high dimentional) numeric form", "type": "idea", "tags": "exploration"}, {"node": "memetic_algorithm", "title": "Memetic Algorithm", "description": "Also known as genetic local search, Lamarckian EAs, Baldwinian EAs, cultual algorithms", "type": "idea", "tags": "evolution, exploration"}, {"node": "pop_based_training", "title": "Population based training", "description": "Training hyperparameters are set by population of evolving learners.", "type": "idea", "tags": "evolution"}, {"node": "q_learning", "title": "Q Learning", "description": "Learning by ", "type": "idea", "tags": "reinforcement"}, {"node": "dqn", "title": "Deep Q Network", "description": "A q-learning approach utlitizing deep neural networks", "type": "algorithm", "tags": "reinforcement"}, {"node": "bootstrap_dqn", "title": "Bootstrap DQN", "description": "A exploration method for DQN based RL based off random value functions", "type": "algorithm", "tags": "reinforcement,exploration"}, {"node": "dithering_exploration", "title": "Dithering exploration", "description": "Exploration strategies which rely on minor perterbations in action space", "type": "idea", "tags": "exploration"}, {"node": "randomized_value_exploration", "title": "Randomized value exploration", "description": "Exploring an entire path based on the guidence of a randomized function", "type": "idea", "tags": "exploration"}, {"node": "alphastar", "title": "AlphaStar", "type": "project", "description": "Deepmind's Starcraft playing engine."}, {"node": "off_policy_learning", "title": "Off policy learning", "description": "The problem of a policy learning from the consequences of actions that a slightly different policy took.", "type": "idea", "tags": "reinforcement"}, {"node": "curriculum_learning", "title": "Curriculum learning", "description": "The idea that learning is fastest when training is on carefully prepared dataset where difficulty increases with abililty. ", "type": "idea", "tags": "learning"}, {"node": "self_play", "title": "Self-play", "description": "In a game, a RL agent can compete with itself to produce learning scenarios", "type": "algorithm", "tags": "learning"}, {"node": "fictitious_self_play", "title": "Fictitious self-play", "description": "Agent plays against itself and archives of its past policies while learning.", "type": "algorithm"}, {"node": "prioritized_fictitious_self_play", "title": "Prioritized fictitious self-play", "description": "Prioritizes play against agents it does poorly against", "type": "algorithm"}, {"node": "ladder_based_play_matching", "title": "Ladder based play matching", "description": "Prioritizes play against agents of similar overall elo", "type": "algorithm"}, {"node": "disk_game", "title": "Disk Game", "description": "A differentiable version of Rock Paper Scisors", "type": "problem"}, {"node": "helmholtz_decomposition", "title": "Helmholtz decomposition", "description": "Any sufficiently smooth, rapidly decaying function in 3d is the sum of a curl and a divergence term.", "type": "idea"}, {"node": "transitive_cyclic_decomposition", "title": "Transitive Cyclic Decomposition", "description": "Any zero-sum differentiable game can be decomposed into a cyclic (counterable) and a transitive (globally better) parts", "type": "idea"}, {"node": "response_to_nash", "title": "Response to the Nash", "description": "Strategies are updated with the best response to the Nash equilibria of the population.", "type": "algorithm"}, {"node": "response_to_rectified_nash", "title": "Response to the rectified Nash", "description": "Each effective agent is trained against agents it beats or ties", "type": "algorithm"}, {"node": "effective_diversity", "title": "Effective Diversity", "description": "A reward based definition of strategic diversity in antisymmetric two player game", "type": "idea"}, {"node": "colonel_blotto", "title": "Colonel Blotto", "description": "Resource allocation game where every player allocates resources to every area and wins in an area if they allocate the most resources.", "type": "problem"}, {"node": "smoothed_symetric_nash_diversity", "title": "Diversity for uniform agents", "description": "Diversity in a population of uniform agents can be obtained by compting the number of 'smoothed' nash in the game.", "type": "idea"}, {"node": "diverse_two_player_multi_agent_games", "title": "Diversity for two player multi-agent games", "description": "", "type": "idea"}, {"node": "nash_matrix_estimation", "title": "N-player game matrix estimation", "description": "", "type": "algorithm"}, {"node": "active_thief_discovery", "title": "Active thief discovery", "description": "A discovery problem of a adversarial agent", "type": "problem"}, {"node": "nash_based_diversity", "title": "Nash based Diversity", "description": "Evaluating diversity based off properties of the nash equilibria of a game", "type": "idea"}, {"node": "count_based_exploration", "title": "Count based exploration", "description": "Exploring unfamiliar states", "type": "idea"}, {"node": "information_maximizing_exploration", "title": "Information maximizing exploration", "description": "Exploration to maximize information generated by the policy", "type": "idea"}, {"node": "DIAYN", "title": "DIAYN", "description": "Diversity Is All You Need -- userpervised RL for skill discovery", "type": "algorithm"}, {"node": "clock_information_exploration", "title": "Clock information exploration", "description": "Continuous time DIAYN", "type": "algorithm"}, {"node": "value_curvature_problem", "title": "Biased Value Estimation Problem", "description": "Values are not accurate around rarely visited points -- leading to overfitting, weird curvatures in value surface and difficult training", "type": "problem"}, {"node": "value_target_optimization", "title": "Value Target Optimization", "description": "Solving the out of sample value problem through expriencing values of different value magnitudes", "type": "idea"}, {"node": "factorized_value_optimization", "title": "Factorized Value Optimization", "description": "Distinct rewards should have distinct values for faster training -- should not be prematurely summed", "type": "idea"}, {"node": "factorized_target_exploration", "title": "Factorized Target Exploration", "description": "A collection of improvements upon DIAYN", "type": "algorithm"}, {"node": "out_of_sample_estimation", "title": "Out of sample estimation", "description": "How does RL deal with novel states during training?", "type": "problem"}, {"node": "double_q_networks", "title": "Double Q Networks", "description": "Conservative estimates in the face of uncertainty by taking the min of two neural networks", "type": "idea"}, {"node": "episilon_greedy", "title": "Epsilon Greedy", "description": "Epsilon greedy chooses random action 1/epsilon of the time", "type": "idea"}, {"node": "guassian_sampling", "title": "Guassian Dithering", "description": "Chooses a action with deterministic mean and fixed guassian distribution", "type": "idea"}, {"node": "entropy_regularization", "title": "Entropy Regularization", "description": "Network chooses mean and variance -- is rewarded for choosing high variance (high entropy)", "type": "idea"}, {"node": "memory", "title": "Memory", "description": "Agents must manage partial observibility by keeping a memory of prior events", "type": "problem"}, {"node": "recurrent_memory", "title": "Recurrent Nerual Memory", "description": "RNNs", "type": "idea"}, {"node": "attentional_memory", "title": "Attentional Neural Memory", "description": "", "type": "idea"}, {"node": "state_based_memory", "title": "State Based Memory", "description": "Using environment state to hold memory (nondifferentiable)", "type": "idea"}]