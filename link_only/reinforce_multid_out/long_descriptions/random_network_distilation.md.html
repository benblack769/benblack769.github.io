<p>Comparing <a href="/#next_state_prediction_curiosity.md">nextstate</a> to this method.</p>

<p><img src="linked_data/nextstate-vs-rnd-stacked-5.svg" alt="nextstate vs rnd" /></p>

<p>This tries to solve the  <a href="/#tv_problem">TV problem</a> because the hash function has the same representation power of the learner, so infinite interesting sequences should not be able to occur. However, it is not clear that this worksâ€¦.</p>

<h3 id="citation">Citation</h3>

<pre><code>@article{DBLP:journals/corr/abs-1810-12894,
  author    = {Yuri Burda and
               Harrison Edwards and
               Amos J. Storkey and
               Oleg Klimov},
  title     = {Exploration by Random Network Distillation},
  journal   = {CoRR},
  volume    = {abs/1810.12894},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.12894},
  archivePrefix = {arXiv},
  eprint    = {1810.12894},
  timestamp = {Thu, 08 Nov 2018 10:57:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-12894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
</code></pre>
