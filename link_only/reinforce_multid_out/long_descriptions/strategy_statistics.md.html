<style>

blockquote {
  padding-left: 1em;
  border-left: 0.2em solid rgba(0,0,0, 0.4);
}
</style>

<p>Deepmind’s StarCraft 2 AI creates strategy statistics with a hand-crafted compilation method, no learning involved.</p>

<h3 id="example">example</h3>

<p>Copied from <a href="https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D">nature paper</a></p>

<blockquote>
  <p>From each replay, we extract a statistic z that encodes each player’s build order defined as the first 20 constructed buildings and units, and cumulative statistics, defined as the units, buildings, effects, and upgrades that were present during a game.</p>
</blockquote>

<h3 id="during-supervised-learning">During supervised learning</h3>

<h3 id="during-reinforcement-learning">During reinforcement learning</h3>

<p>When a new agent is created, it is supervised based off that statistic throughout its run, i.e. the agent is penalized for not following the statistics.</p>

<p>One example is build order. If the agent ends up choosing a different build order, then it its penalized strategy statistic’s build order, it is penalized with a hand-crafted loss.</p>
